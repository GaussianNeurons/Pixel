{"cells":[{"cell_type":"code","source":["# Generate NFTs using Deep Convolutional Generative Adversarial Networks\n# Taken from the following medium blog by Bao Tram Duong: \n# https://medium.com/mlearning-ai/generate-nft-cryptopunks-with-deep-convolutional-generative-adversarial-network-dcgan-db35f0a1adb4"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"71f40166-fc2b-4a50-a4b7-fd6931ab12d6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[1]: 1</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[1]: 1</div>"]}}],"execution_count":0},{"cell_type":"code","source":["!pip install opencv-python\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport glob\nimport os\nimport random\nfrom tqdm import tqdm\nimport cv2 as cv\nimport PIL\nfrom PIL import Image\n!pip install plotly\nimport plotly.express as px\nfrom IPython import display\n\nimport tensorflow as tf\nfrom tensorflow import keras\n\nimport torch\nfrom torchvision import datasets\nfrom torchvision import transforms\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"705f679d-1c60-4c50-9147-8f5e5e1a3dc3"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Requirement already satisfied: plotly in /databricks/python3/lib/python3.8/site-packages (5.5.0)\r\nRequirement already satisfied: six in /databricks/python3/lib/python3.8/site-packages (from plotly) (1.15.0)\r\nRequirement already satisfied: tenacity&gt;=6.2.0 in /databricks/python3/lib/python3.8/site-packages (from plotly) (6.2.0)\r\n<span class=\"ansi-yellow-fg\">WARNING: You are using pip version 21.0.1; however, version 22.0.4 is available.\r\nYou should consider upgrading via the &#39;/databricks/python3/bin/python -m pip install --upgrade pip&#39; command.</span>\r\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Requirement already satisfied: plotly in /databricks/python3/lib/python3.8/site-packages (5.5.0)\r\nRequirement already satisfied: six in /databricks/python3/lib/python3.8/site-packages (from plotly) (1.15.0)\r\nRequirement already satisfied: tenacity&gt;=6.2.0 in /databricks/python3/lib/python3.8/site-packages (from plotly) (6.2.0)\r\n<span class=\"ansi-yellow-fg\">WARNING: You are using pip version 21.0.1; however, version 22.0.4 is available.\r\nYou should consider upgrading via the &#39;/databricks/python3/bin/python -m pip install --upgrade pip&#39; command.</span>\r\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Set the directories for the training images (NFTs)\nimage_dir = \"/dbfs/FileStore/nft-hackathon/training_imgs/imgs\"\nimage_root = \"/dbfs/FileStore/nft-hackathon/training_imgs/\"\ndata_dir = \"/dbfs/FileStore/nft-hackathon/training_imgs/\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"291f5afe-f313-44eb-a65c-de4e7ab4b46c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["#look at 100 samples\n#define number of rows and cols\nno_plots = 10*10\n\n#define path\nimages = glob.glob(\"/dbfs/FileStore/nft-hackathon/training_imgs/imgs/*.png\")\n\nplt.rcParams['figure.figsize'] = (30, 30)\nplt.subplots_adjust(wspace=0, hspace=0)\n\nprint(\"Sample 100 CryptoPunks\")\nfor idx,image in enumerate(images[:no_plots]):\n    sample_img = cv.imread(image)\n    plt.subplot(10, 10, idx+1)\n    plt.axis('off')\n    plt.imshow(cv.cvtColor(sample_img,cv.COLOR_BGR2RGB)) #covert color space\nplt.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8eda60ee-5c45-4152-914d-d370b01bf23f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["def print_tensor_images(images_tensor):\n    \n    '''\n    Function for visualizing images: Given a tensor of images, prints the images.\n    '''\n        \n    plt.rcParams['figure.figsize'] = (15, 15)\n    plt.subplots_adjust(wspace=0, hspace=0)\n    \n    images_tensor = images_tensor.to('cpu')\n    npimgs = images_tensor.detach().numpy()\n    \n    no_plots = len(images_tensor)\n\n    for idx,image in enumerate(npimgs):\n        plt.subplot(1, 8, idx+1)\n        plt.axis('off')\n        #dnorm\n        image = image * 0.5 + 0.5\n        plt.imshow(np.transpose(image, (1, 2, 0)))\n        \n    plt.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0aa767ac-ae55-42ae-ab9b-3a9ea09a0e51"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["#helper display function\ndef tensor_imshow(img,dnorm=True):\n    img = img.to('cpu')\n    npimg = img.detach().numpy()\n    if dnorm:\n        npimg = npimg*0.5+0.5\n    plt.figure(figsize=(3, 3))\n    plt.axis('off')\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n    plt.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e2a4c6de-9c6d-4484-a8f1-758926d6e238"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def get_dataloader(batch_size,           #batch size during training\n                   image_size,           #spatial size of training images\n                   data_dir=image_dir,   #root directory for dataset\n                   num_workers=3):       #number of sub-processes\n    \n    stats = (0.5, 0.5, 0.5), (0.5, 0.5, 0.5) \n    \n    #create transformer to transform images\n    transform = transforms.Compose([transforms.Resize((image_size, image_size)),  #resize\n                                    transforms.ToTensor(),                        #convert to tensor\n                                    transforms.Normalize(*stats)])                #normalize to be between -1 and 1\n    \n    #create the dataset\n    dataset = datasets.ImageFolder(root=data_dir,\n                                   transform=transform)\n    \n    #create the dataloader\n    data_loader = torch.utils.data.DataLoader(dataset,\n                                              batch_size=batch_size,\n                                              shuffle=True,\n                                              num_workers=num_workers,\n                                              pin_memory=True)\n    \n    return data_loader"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fa39a436-f5f2-47a1-9713-292e5ecddb08"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["#test dataloader\nbatch_size, image_size = 5, 24\n\ntrain_loader = get_dataloader(batch_size,\n                              image_size,\n                              image_root)\n\ndataiter = iter(train_loader) #dataloader is an iterator\n\nimg,_ = next(dataiter)\nsample_img = img[-1]\n\n#display tensor image\ntensor_imshow(sample_img)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"342850f7-c2c8-4373-87be-ae6c54c298e3"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAALEAAACxCAYAAACLKVzFAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAADA0lEQVR4nO3dIY4UURRAUYogWAQ4BAmShBWQ4JE4NoDDEHaAYwM4QLECVkCCJEHgZhaB+2xgqpjU1HT17T5HdmUyX9w88VK/expj3IGyu3sfAG5KxOSJmDwRkydi8kRM3r2lh9M02b9xNMYY01Wfm8TkiZg8EZMnYvJETJ6IyRMxeSImT8TkiZg8EZMnYvJETJ6IyRMxeSImT8TkiZg8EZMnYvJETJ6IyRMxeSImT8TkiZg8EZMnYvJETJ6IyRMxeSImT8TkiZg8EZMnYvJETJ6IyRMxeYu/Y1fw58GDvY+Q8ejycu8j3AqTmDwRkydi8kRMnojJEzF50xhj/uE0zT+EAxtjTFd9bhKTJ2LyREyeiMkTMXkiJk/E5ImYPBGTJ2LyREyeiMnL37Fb8u3D672PcHAv337a+wgHZxKTJ2LyREyeiMkTMXkiJk/E5ImYPBGTJ2LyREyeiMkTMXkn/RbbGr9/fl/1d4+fPt/4JOvPcm5MYvJETJ6IyRMxeSImT8Tk5VdsW18Gfff1YvbZl/cvNv1f/zN3luVzzJ//VJnE5ImYPBGTJ2LyREyeiMnLr9i2dkzf33ZMZzlmJjF5IiZPxOSJmDwRkydi8vIrtqWv97eiOg8mMXkiJk/E5ImYPBGTl99OLPl78WP22f2Hzw52jrU/kGi7cj0mMXkiJk/E5ImYPBGTJ2LyTnrFtmRu/XYbq7etV2VLq8NzZBKTJ2LyREyeiMkTMXkiJk/E5ImYPBGTJ2LyREyeiMkTMXkn/Rbb0htpc2+Crb1cuvYy6BIXRa/HJCZPxOSJmDwRkydi8kRM3kmv2A7JOmw/JjF5IiZPxOSJmDwRkydi8s52xTb3RtrSW2zH8h1orz7+2vsIR8UkJk/E5ImYPBGTJ2LyznY7MWftPbrPb55sfhZbiOsxickTMXkiJk/E5ImYPBGTZ8W2Eeuw/ZjE5ImYPBGTJ2LyREyeiMkTMXkiJk/E5ImYPBGTJ2LyREzeNMbY+wxwIyYxeSImT8TkiZg8EZMnYvL+AaD1UySf6s+EAAAAAElFTkSuQmCC","removedWidgets":[],"addedWidgets":{},"metadata":{"imageDimensions":{"width":177,"height":177}},"type":"image","arguments":{}},"image/png":{"width":177,"height":177}},"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAALEAAACxCAYAAACLKVzFAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAADA0lEQVR4nO3dIY4UURRAUYogWAQ4BAmShBWQ4JE4NoDDEHaAYwM4QLECVkCCJEHgZhaB+2xgqpjU1HT17T5HdmUyX9w88VK/expj3IGyu3sfAG5KxOSJmDwRkydi8kRM3r2lh9M02b9xNMYY01Wfm8TkiZg8EZMnYvJETJ6IyRMxeSImT8TkiZg8EZMnYvJETJ6IyRMxeSImT8TkiZg8EZMnYvJETJ6IyRMxeSImT8TkiZg8EZMnYvJETJ6IyRMxeSImT8TkiZg8EZMnYvJETJ6IyRMxeYu/Y1fw58GDvY+Q8ejycu8j3AqTmDwRkydi8kRMnojJEzF50xhj/uE0zT+EAxtjTFd9bhKTJ2LyREyeiMkTMXkiJk/E5ImYPBGTJ2LyREyeiMnL37Fb8u3D672PcHAv337a+wgHZxKTJ2LyREyeiMkTMXkiJk/E5ImYPBGTJ2LyREyeiMkTMXkn/RbbGr9/fl/1d4+fPt/4JOvPcm5MYvJETJ6IyRMxeSImT8Tk5VdsW18Gfff1YvbZl/cvNv1f/zN3luVzzJ//VJnE5ImYPBGTJ2LyREyeiMnLr9i2dkzf33ZMZzlmJjF5IiZPxOSJmDwRkydi8vIrtqWv97eiOg8mMXkiJk/E5ImYPBGTl99OLPl78WP22f2Hzw52jrU/kGi7cj0mMXkiJk/E5ImYPBGTJ2LyTnrFtmRu/XYbq7etV2VLq8NzZBKTJ2LyREyeiMkTMXkiJk/E5ImYPBGTJ2LyREyeiMkTMXkn/Rbb0htpc2+Crb1cuvYy6BIXRa/HJCZPxOSJmDwRkydi8kRM3kmv2A7JOmw/JjF5IiZPxOSJmDwRkydi8s52xTb3RtrSW2zH8h1orz7+2vsIR8UkJk/E5ImYPBGTJ2LyznY7MWftPbrPb55sfhZbiOsxickTMXkiJk/E5ImYPBGTZ8W2Eeuw/ZjE5ImYPBGTJ2LyREyeiMkTMXkiJk/E5ImYPBGTJ2LyREzeNMbY+wxwIyYxeSImT8TkiZg8EZMnYvL+AaD1UySf6s+EAAAAAElFTkSuQmCC"}}],"execution_count":0},{"cell_type":"code","source":["class Generator(nn.Module):       #signals neural network\n    def __init__(self, \n                 z_dim=100,      #noise vector\n                 im_chan=3,      #color chanel, 3 for red green blue\n                 hidden_dim=64): #spatial size of feature map (conv)\n        \n        super(Generator, self).__init__()\n        self.z_dim = z_dim\n        self.im_chan = im_chan\n        self.hidden_dim = hidden_dim\n        \n        self.generator_cnn = nn.Sequential(self.make_gen_block(z_dim, hidden_dim*8, stride=1, padding=0),   \n                                           #(64*8) x 4 x 4\n                                           self.make_gen_block(hidden_dim*8, hidden_dim*4),                           \n                                           #(64*4) x 8 x 8\n                                           self.make_gen_block(hidden_dim*4, hidden_dim*2),                           \n                                           #(64*2) x 16 x 16\n                                           self.make_gen_block(hidden_dim*2, hidden_dim),                             \n                                           #(64) x 32 x 32\n                                           self.make_gen_block(hidden_dim, im_chan, final_layer=True))\n    \n    def make_gen_block(self, \n                       im_chan,     #image dimension\n                       op_chan,     #output dimension\n                       kernel_size=4, \n                       stride=2, \n                       padding=1, \n                       final_layer=False): \n        \n        layers = []\n        #de-convolutional layer\n        layers.append(nn.ConvTranspose2d(im_chan,     \n                                         op_chan, \n                                         kernel_size, \n                                         stride, \n                                         padding, \n                                         bias=False))\n        \n        if not final_layer:\n            layers.append(nn.BatchNorm2d(op_chan))\n            layers.append(nn.LeakyReLU(0.2))\n        else:\n            layers.append(nn.Tanh())\n        \n        return nn.Sequential(*layers)\n    \n    def forward(self,noise):\n        x = noise.view(-1,self.z_dim,1,1)\n        return self.generator_cnn(x)\n\n    def get_noise(n_samples, \n                  z_dim, \n                  device='cpu'):\n        return torch.randn(n_samples, \n                           z_dim, \n                           device=device)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b4d3f07b-4a6e-49d2-8b18-da2d6ae49cb4"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["#test Generator\nnoise = Generator.get_noise(n_samples=5,\n                            z_dim=100)\n\ng = Generator(z_dim=100,\n              im_chan=3,\n              hidden_dim=64)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1ab4360f-e2ed-49ef-be1c-76e7d0c8ec58"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["class Discriminator(nn.Module):\n    def __init__(self, \n                 im_chan=3,       #image channels, 3 for red green blue\n                 conv_dim=64,     #spatial dimension of feature map\n                 image_size=64):  #spatial size of training images\n        \n        super(Discriminator, self).__init__()\n        self.image_size = image_size\n        self.conv_dim = conv_dim\n        \n        self.disc_cnn = nn.Sequential(self.make_disc_block(im_chan, conv_dim),\n                                      self.make_disc_block(conv_dim, conv_dim*2),\n                                      self.make_disc_block(conv_dim*2, conv_dim*4),\n                                      self.make_disc_block(conv_dim*4, conv_dim*8),\n                                      #no need a sigmoid here since it is included in the loss function\n                                      self.make_disc_block(conv_dim*8, 1, padding=0, final_layer=True)) \n        \n        \n    def make_disc_block(self,\n                        im_chan,\n                        op_chan,\n                        kernel_size=4,\n                        stride=2,\n                        padding=1,\n                        final_layer=False):\n        layers = []\n        layers.append(nn.Conv2d(im_chan,\n                                op_chan,\n                                kernel_size,\n                                stride,\n                                padding,\n                                bias=False))\n        \n        if not final_layer:\n            layers.append(nn.BatchNorm2d(op_chan))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n        \n        return nn.Sequential(*layers)\n    \n    #given an image tensor, returns a 1-dimension tensor representing fake/real\n    def forward(self,image):\n        pred = self.disc_cnn(image)\n        pred = pred.view(image.size(0),-1)\n        return pred\n    \n    def _get_final_feature_dimention(self):\n        final_width_height = (self.image_size //  2**len(self.disc_cnn))**2\n        final_depth = self.conv_dim * 2**(len(self.disc_cnn)-1)\n        return final_depth*final_width_height"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"32ddc6fe-c703-4ac7-aa07-021ee2450646"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["#test Discriminator\nd = Discriminator(im_chan=3,\n                  conv_dim=64,\n                  image_size=64)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7238d679-b33e-4d2d-9a5c-a62132257fe8"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["#custom weights initialization to randomly initialize all weights\n#mean=0, stdev=0.2\ndef weights_init_normal(m):\n    \n    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n        torch.nn.init.normal_(m.weight, 0.0, 0.02) \n        \n    if isinstance(m, nn.BatchNorm2d):\n        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n        torch.nn.init.constant_(m.bias, 0)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"18023ba8-8b5e-48bb-9679-d937b6b90032"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def real_loss(D_out,device='cuda'):\n    \n    #initialize BCELoss function\n    criterion = nn.BCEWithLogitsLoss()\n    \n    #batch size\n    #batch_size = D_out.size(0)\n    batch_size = D_out.shape\n    \n    #labels will be used when calculating the losses\n    #real labels = 1 and lable smoothing => 0.9\n    labels = torch.ones(batch_size, device=device)*0.9 \n    \n    #loss = criterion(D_out.squeeze(), labels)\n    loss = criterion(D_out, labels)\n    return loss"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"56589140-e32f-4df3-86a2-225a0b600606"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def fake_loss(D_out, device='cuda'):\n    \n    #initialize BCELoss function\n    criterion = nn.BCEWithLogitsLoss()\n    \n    #batch size\n    #batch_size = D_out.size(0)\n    batch_size = D_out.shape\n    \n    #labels will be used when calculating the losses\n    #fake labels = 0\n    labels = torch.zeros(batch_size,\n                         device=device) \n    \n    #loss = criterion(D_out.squeeze(), labels)\n    loss = criterion(D_out, labels)\n    return loss"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c5c5dd41-b068-42b4-a292-5e472e4fbc7f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def train(D, G, \n          n_epochs,\n          dataloader,\n          d_optimizer,\n          g_optimizer,\n          z_dim,\n          print_every=50,\n          device='cuda'):\n    \n    #to keep track of the generator’s learning progression, \n    #we will generate a fixed batch of latent vectors that are drawn from a Gaussian distribution   \n    sample_size=8\n    fixed_z = Generator.get_noise(n_samples=sample_size,\n                                  z_dim=z_dim,\n                                  device=device)\n    \n    for epoch in range(1,n_epochs+1):\n        #use dataloader to fetch batches\n        for batch_i,(real_images,_) in enumerate(dataloader):\n            batch_size = real_images.size(0)\n            real_images = real_images.to(device)\n            \n            #Part 1: Train the Discriminator ========================================================\n            #goal: to maximize the probability of correctly classifying a given input as real or fake\n            \n            #zero out the gradients before backpropagation\n            d_optimizer.zero_grad()\n            \n            ##classify all-real batch\n            d_real_op = D(real_images) #average output (across the batch) of the discriminator\n            d_real_loss = real_loss(d_real_op,\n                                    device=device)\n            \n            #train with all-fake batch\n            noise = Generator.get_noise(n_samples=batch_size,\n                                        z_dim=z_dim,\n                                        device=device)\n            fake_images = G(noise)\n            \n            #classify all-fake batch\n            d_fake_op = D(fake_images) #average output (across the batch) of the generator\n            d_fake_loss = fake_loss(d_fake_op,\n                                    device=device)\n            \n            #total loss\n            d_loss = d_real_loss + d_fake_loss\n            \n            #update gradients\n            d_loss.backward()\n            #update optimizer\n            d_optimizer.step()\n            \n            #Part 2: Train the Generator ==============================================================\n            #zero out the gradients before backpropagation\n            g_optimizer.zero_grad()\n            noise = Generator.get_noise(n_samples=batch_size,\n                                        z_dim=z_dim,\n                                        device=device)\n            \n            #use discriminator to classify generator's output\n            g_out = G(noise)\n            d_out = D(g_out)\n            \n            g_loss = real_loss(d_out, \n                               device=device) \n            #update gradients\n            g_loss.backward()\n            #update optimizer\n            g_optimizer.step()\n        \n        print('Epoch [{:5d}/{:5d}] | d_loss: {:6.4f} | g_loss: {:6.4f}'.format(epoch, \n                                                                               n_epochs, \n                                                                               d_loss.item(),  #keep track of loss\n                                                                               g_loss.item())) #keep track of loss\n        \"\"\"\n        if (epoch % print_every == 0):\n            G.eval()\n            sample_image = G(fixed_z)\n            print_tensor_images(sample_image)\n            G.train()\n        \"\"\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4d1c4ec7-574e-4d4e-9c0c-4ce5768ccb25"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["#hyperparameters\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"Device is \", device)\n\n#incorrect hyperparameter settings lead to mode collapse\n#we will follow Goodfellow’s paper\nz_dim = 100       #noise\nbeta_1 = 0.5      #as specified in the original DCGAN paper\nbeta_2 = 0.999 \nlr = 0.0002       #as specified in the original DCGAN paper\nn_epochs = 100\nbatch_size = 128\nimage_size = 64"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"67d14fc0-14f0-40ab-a09c-93bd0f79c7e0"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Device is  cuda\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Device is  cuda\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["#initialize generator\ngenerator = Generator(z_dim, \n                      im_chan=3, \n                      hidden_dim=64).to(device)\n\n\n#initialize discriminator\ndiscriminator = Discriminator(im_chan=3, \n                              conv_dim=64, \n                              image_size=image_size).to(device)\n\n#setup Adam optimizers for generator\ng_optimizer = optim.Adam(generator.parameters(), \n                         lr=lr, \n                         betas=(beta_1, beta_2))\n\n#setup Adam optimizers for discriminator\nd_optimizer = optim.Adam(discriminator.parameters(), \n                         lr=lr, \n                         betas=(beta_1, beta_2))\n\n#setup dataloader\ndataloader = get_dataloader(batch_size, \n                            image_size, \n                            image_root)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"626d7a83-415b-42a6-92b5-49c878dbb8ea"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["#start training\n%time\nn_epochs = 100\ntrain(discriminator,\n      generator,\n      n_epochs,\n      dataloader,\n      d_optimizer,\n      g_optimizer,\n      z_dim,\n      print_every=1,\n      device=device)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"95f03361-9ef0-4bdd-b04f-85a81475fcb9"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">CPU times: user 3 µs, sys: 1e+03 ns, total: 4 µs\nWall time: 7.15 µs\nEpoch [    1/  100] | d_loss: 0.3596 | g_loss: 5.4347\nEpoch [    2/  100] | d_loss: 1.3856 | g_loss: 5.9266\nEpoch [    3/  100] | d_loss: 0.7076 | g_loss: 1.9980\nEpoch [    4/  100] | d_loss: 0.4969 | g_loss: 2.9241\nEpoch [    5/  100] | d_loss: 0.5413 | g_loss: 3.7571\nEpoch [    6/  100] | d_loss: 0.3989 | g_loss: 2.7720\nEpoch [    7/  100] | d_loss: 0.7019 | g_loss: 2.5836\nEpoch [    8/  100] | d_loss: 0.3765 | g_loss: 2.9196\nEpoch [    9/  100] | d_loss: 0.6570 | g_loss: 3.0733\nEpoch [   10/  100] | d_loss: 1.7446 | g_loss: 6.4490\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">CPU times: user 3 µs, sys: 1e+03 ns, total: 4 µs\nWall time: 7.15 µs\nEpoch [    1/  100] | d_loss: 0.3596 | g_loss: 5.4347\nEpoch [    2/  100] | d_loss: 1.3856 | g_loss: 5.9266\nEpoch [    3/  100] | d_loss: 0.7076 | g_loss: 1.9980\nEpoch [    4/  100] | d_loss: 0.4969 | g_loss: 2.9241\nEpoch [    5/  100] | d_loss: 0.5413 | g_loss: 3.7571\nEpoch [    6/  100] | d_loss: 0.3989 | g_loss: 2.7720\nEpoch [    7/  100] | d_loss: 0.7019 | g_loss: 2.5836\nEpoch [    8/  100] | d_loss: 0.3765 | g_loss: 2.9196\nEpoch [    9/  100] | d_loss: 0.6570 | g_loss: 3.0733\nEpoch [   10/  100] | d_loss: 1.7446 | g_loss: 6.4490\n</div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Cancelled","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["#sample generation\ngenerator.to(device)\ngenerator.eval()\nsample_size=8\nfor i in range(5):\n    fixed_z = Generator.get_noise(n_samples=sample_size,z_dim=z_dim,device=device)    \n    sample_image = generator(fixed_z)\n    print_tensor_images(sample_image)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"95b31ce4-0ab1-40a3-aa97-b704e464de25"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"DCGAN_NFT","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":140039482329063}},"nbformat":4,"nbformat_minor":0}
